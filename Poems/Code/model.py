# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eMQeTYpXHyU7gaitJdc6RXyJ-z_99EUi
"""

import tensorflow as tf
from google.colab import drive
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
print(tf.__version__)

# drive.mount('/content/drive')

def preprocess_text_file():
  file_name_base = 'poem-'
  file_extension = '.txt'

  process_name = 'processed'
  i = 1
  for i in range(1, self.n_poems + 1):
    processed_poem = []
    # get file name, file_name_format - 'kipling-1.txt'
    read_file_name = file_name_base + str(i) + file_extension
    file_read = open(read_file_name, 'r')
    
    # read file, append to a list
    for x in file_read:
      processed_poem.append(x)
      
    # close the file
    file_read.close()

    # convert to string
    processed_poem = str(processed_poem)

    # processed file name - 'processed-1.txt'    
    write_file_name = process_name + '-' + str(i) + file_extension

    file_write = open(write_file_name, 'w')

    # write string to file
    file_write.write(processed_poem)
    file_write.close()

def get_training_data(base_file_name):
  file_read = open(base_file_name, 'r')

  data_list = []
  for x in file_read:
    data_list.append(x)

  train_data = str(data_list)
  return train_data

def alternative_get_data():
  processed_1  = str("If you can keep your head when all about you \n, Are losing theirs and blaming it on you; \n, If you can trust yourself when all men doubt you, \n, But make allowance for their doubting too: \n, If you can wait and not be tired by waiting, \n, Or, being lied about, dont deal in lies, \n, Or being hated dont give way to hating, \n, And yet dont look too good, nor talk too wise; \n,  \n, If you can dream - and not make dreams your master; \n, If you can think - and not make thoughts your aim, \n, If you can meet with Triumph and Disaster \n, And treat those two impostors just the same:. \n, If you can bear to hear the truth youve spoken \n, Twisted by knaves to make a trap for fools, \n, Or watch the things you gave your life to, broken, \n, And stoop and buildem up with worn-out tools; \n,  \n, If you can make one heap of all your winnings \n, And risk it on one turn of pitch-and-toss, \n, And lose, and start again at your beginnings, \n, And never breathe a word about your loss: \n, If you can force your heart and nerve and sinew \n, To serve your turn long after they are gone, \n, And so hold on when there is nothing in you \n, Except the Will which says to them: Hold on! \n,  \n, If you can talk with crowds and keep your virtue, \n, Or walk with Kings - nor lose the common touch, \n, If neither foes nor loving friends can hurt you, \n, If all men count with you, but none too much: \n, If you can fill the unforgiving minute \n, With sixty seconds worth of distance run, \n, Yours is the Earth and everything thats in it, \n, And - which is more - youll be a Man, my son! \n ")
  processed_2 = str("Do not stand at my grave and weep \n, I am not there; I do not sleep. \n, I am a thousand winds that blow, \n, I am the diamond glints on snow, \n, I am the sun on ripened grain, \n, I am the gentle autumn rain. \n, When you awaken in the mornings hush \n, I am the swift uplifting rush \n, Of quiet birds in circled flight. \n, I am the soft stars that shine at night. \n, Do not stand at my grave and cry, \n, I am not there; I did not die. \n ")
  processed_3 = str("Death is nothing at all. \n, It does not count. \n, I have only slipped away into the next room. \n, Nothing has happened. \n,  \n, Everything remains exactly as it was. \n, I am I, and you are you, \n, and the old life that we lived so fondly together is untouched, unchanged. \n, Whatever we were to each other, that we are still. \n,  \n, Call me by the old familiar name. \n, Speak of me in the easy way which you always used. \n, Put no difference into your tone. \n, Wear no forced air of solemnity or sorrow. \n,  \n, Laugh as we always laughed at the little jokes that we enjoyed together. \n, Play, smile, think of me, pray for me. \n, Let my name be ever the household word that it always was. \n, Let it be spoken without an effort, without the ghost of a shadow upon it. \n,  \n, Life means all that it ever meant. \n, It is the same as it ever was. \n, There is absolute and unbroken continuity. \n, What is this death but a negligible accident? \n,  \n, Why should I be out of mind because I am out of sight? \n, I am but waiting for you, for an interval, \n, somewhere very near, \n, just round the corner. \n,  \n, All is well. \n, Nothing is hurt; nothing is lost. \n, One brief moment and all will be as it was before. \n, How we shall laugh at the trouble of parting when we meet again! \n ")
  processed_4 = str("Ah, sunflower, weary of time, \n, Who countest the steps of the sun; \n, Seeking after that sweet golden clime \n, Where the travellers journey is done; \n,  \n, Where the Youth pined away with desire, \n, And the pale virgin shrouded in snow, \n, Arise from their graves, and aspire \n, Where my Sunflower wishes to go! ")
  processed_5 = str("Merry, merry sparrow! \n, Under leaves so green \n, A happy blossom \n, Sees you, swift as arrow, \n, Seek your cradle narrow, \n, Near my bosom. \n, Pretty, pretty robin! \n, Under leaves so green \n, A happy blossom \n, Hears you sobbing, sobbing, \n, Pretty, pretty robin, \n, Near my bosom. ")
  processed_6 = str("Bring me an axe and spade, \n, Bring me a winding sheet; \n, When I my grave have made, \n, Let winds and tempests beat: \n, Then down I lie, as cold as clay. \n, True love doth pass away! ")
  processed_7 = str("The Mercy, Pity, Peace, and Love  \n, All pray in their distress; \n, And to these virtues of delight \n, Return their thankfulness. \n,  \n, For Mercy, Pity, Peace, and Love \n, Is God, our Father dear, \n, And Mercy, Pity, Peace, and Love \n, Is man, His child and care. \n,  \n, For Mercy has a human heart, \n, Pity a human face, \n, And Love, the human form divine, \n, And Peace, the human dress. \n,  \n, Then every man, of every clime, \n, That prays in his distress, \n, Prays to the human form divine, \n, Love, Mercy, Pity, Peace. \n,  \n, For all must love the human form, \n, In heathen, Turk, or Jew; \n, Where Mercy, Love, and Pity dwell \n, There God is dwelling too ")
  processed_8 = str("Once a dream did weave a shade \n, Oer my angel-guarded bed, \n, That an emmet lost its way \n, Where on grass methought I lay. \n,  \n, Troubled, wildered, and forlorn, \n, Dark, benighted, travel-worn, \n, Over many a tangled spray, \n, All heart-broke, I heard her say: \n,  \n, O my children! do they cry, \n, Do they hear their father sigh? \n, Now they look abroad to see, \n, Now return and weep for me. \n,  \n, Pitying, I dropped a tear: \n, But I saw a glow-worm near, \n, Who replied, What wailing wight \n, Calls the watchman of the night? \n,  \n, I am set to light the ground, \n, While the beetle goes his round: \n, Follow now the beetles hum; \n, Little wanderer, hie thee home! ")
  processed_9 = str("Earth raised up her head \n, From the darkness dread and drear, \n, Her light fled, \n, Stony, dread, \n, And her locks covered with grey despair. \n,  \n, Prisoned on watery shore, \n, Starry jealousy does keep my den \n, Cold and hoar; \n, Weeping oer, \n, I hear the father of the ancient men. \n,  \n, Selfish father of men! \n, Cruel, jealous, selfish fear! \n, Can delight, \n, Chained in night, \n, The virgins of youth and morning bear. \n,  \n, Does spring hide its joy, \n, When buds and blossoms grow? \n, Does the sower \n, Sow by night, \n, Or the ploughman in darkness plough? \n,  \n, Break this heavy chain, \n, That does freeze my bones around! \n, Selfish, vain, \n, Eternal bane, \n, That free love with bondage bound. ")
  processed_10 = str("Can I see anothers woe, \n, And not be in sorrow too? \n, Can I see anothers grief, \n, And not seek for kind relief? \n,  \n, Can I see a falling tear, \n, And not feel my sorrows share? \n, Can a father see his child \n, Weep, nor be with sorrow filled? \n,  \n, Can a mother sit and hear \n, An infant groan, an infant fear? \n, No, no! never can it be! \n, Never, never can it be! \n,  \n, And can He who smiles on all \n, Hear the wren with sorrows small, \n, Hear the small birds grief and care, \n, Hear the woes that infants bear - \n,  \n, And not sit beside the nest, \n, Pouring pity in their breast, \n, And not sit the cradle near, \n, Weeping tear on infants tear? \n,  \n, And not sit both night and day, \n, Wiping all our tears away? \n, O no! never can it be! \n, Never, never can it be! \n,  \n, He doth give His joy to all: \n, He becomes an infant small, \n, He becomes a man of woe, \n, He doth feel the sorrow too. \n,  \n, Think not thou canst sigh a sigh, \n, And thy Maker is not by: \n, Think not thou canst weep a tear, \n, And thy Maker is not near. \n,  \n, O He gives to us His joy, \n, That our grief He may destroy: \n, Till our grief is fled and gone \n, He doth sit by us and moan. \n, Can I see anothers woe, \n, And not be in sorrow too? \n, Can I see anothers grief, \n, And not seek for kind relief? \n,  \n, Can I see a falling tear, \n, And not feel my sorrows share? \n, Can a father see his child \n, Weep, nor be with sorrow filled? \n,  \n, Can a mother sit and hear \n, An infant groan, an infant fear? \n, No, no! never can it be! \n, Never, never can it be! \n,  \n, And can He who smiles on all \n, Hear the wren with sorrows small, \n, Hear the small birds grief and care, \n, Hear the woes that infants bear - \n,  \n, And not sit beside the nest, \n, Pouring pity in their breast, \n, And not sit the cradle near, \n, Weeping tear on infants tear? \n,  \n, And not sit both night and day, \n, Wiping all our tears away? \n, O no! never can it be! \n, Never, never can it be! \n,  \n, He doth give His joy to all: \n, He becomes an infant small, \n, He becomes a man of woe, \n, He doth feel the sorrow too. \n,  \n, Think not thou canst sigh a sigh, \n, And thy Maker is not by: \n, Think not thou canst weep a tear, \n, And thy Maker is not near. \n,  \n, O He gives to us His joy, \n, That our grief He may destroy: \n, Till our grief is fled and gone \n, He doth sit by us and moan. ")
  train_data = processed_1 + processed_2 + processed_3 + processed_4 + processed_5 + processed_6 + processed_7 + processed_8 + processed_9 + processed_10 
  return train_data

def preprocess_training_data(train_data):
  tokenizer = Tokenizer()

  corpus = train_data.lower().split("\n")
  tokenizer.fit_on_texts(corpus)
  total_words = len(tokenizer.word_index) + 1

  input_sequences = []

  for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
      n_gram_sequence = token_list[:i+1]
      input_sequences.append(n_gram_sequence)

  # pad sequences
  max_sequence_len = max([len(x) for x in input_sequences])
  input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))
  return input_sequences, max_sequence_len, total_words, tokenizer

def create_predictors(input_sequences, total_words):
  xs, labels = input_sequences[:,:-1], input_sequences[:,-1]
    
  ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)
  return xs, ys, labels

def create_model(max_sequence_len, total_words):
  model = Sequential()
  model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))
  model.add(Bidirectional(LSTM(150)))
  model.add(Dense(total_words, activation='softmax'))
  adam = Adam(lr=0.01)
  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
  print(model.summary())
  return model

  
def run_model(xs, ys, model, epochs = 20):
  history = model.fit(xs, ys, epochs = epochs, verbose=1)
  return history

  
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.show()

def predict_poem(seed_text, next_words, model, tokenizer):
  for _ in range(next_words):
	  token_list = tokenizer.texts_to_sequences([seed_text])[0]
	  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	  predicted = model.predict_classes(token_list, verbose=0)
	  output_word = ""
	  for word, index in tokenizer.word_index.items():
		  if index == predicted:
			  output_word = word
			  break
	  seed_text += " " + output_word
  print(seed_text)

train_data = alternative_get_data()

input_sequences, max_sequence_len, total_words, tokenizer  = preprocess_training_data(train_data)

xs, ys, labels = create_predictors(input_sequences, total_words)

model = create_model(max_sequence_len, total_words)

history = run_model(xs, ys, model)

predict_poem('Life', 20, model, tokenizer)

# create model
model_doubleLSTM = Sequential()
model_doubleLSTM.add(Embedding(total_words, 100, input_length=max_sequence_len-1))
model_doubleLSTM.add(Bidirectional(LSTM(150, return_sequences = True)))
model_doubleLSTM.add(Bidirectional(LSTM(50)))#model_doubleLSTM.add(Dense(total_words, activation='softmax'))
model_doubleLSTM.add(Dense(total_words, activation='softmax'))
adam = Adam(lr=0.01)
model_doubleLSTM.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
print(model_doubleLSTM.summary())
# return model

  
# def run_model(xs, ys, model, epochs = 20):
history = model_doubleLSTM.fit(xs, ys, epochs = 30, verbose=1)
#return history

predict_poem('Life', 20, model_doubleLSTM, tokenizer)

# create model
model_2 = Sequential()
model_2.add(Embedding(total_words, 100, input_length=max_sequence_len-1))
model_2.add(Bidirectional(LSTM(150, return_sequences = True)))
model_2.add(Bidirectional(LSTM(50)))
model_2.add(Dense(512, activation='relu'))
model_2.add(Dense(total_words, activation='softmax'))
adam = Adam(lr=0.01)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
print(model_2.summary())
# return model

  
# def run_model(xs, ys, model, epochs = 20):
history = model_doubleLSTM.fit(xs, ys, epochs = 10, verbose=1)
#return history

predict_poem('The world', 20, model_2, tokenizer)

